{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94138b64-9791-4e5d-8878-6ad1df49ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, anomaly detection, LLM-based rule generation, and file handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from transformers import pipeline\n",
    "import datetime\n",
    "import os  # Added for directory creation in Cell 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b0854b-b6c3-4176-bb2c-7aad58cc9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default profiling rules based on regulatory instructions, to be used for data validation\n",
    "# Includes manually implemented rules from Cell 12 for Transaction_Amount vs Reported_Amount and Currency\n",
    "default_profiling_rules = {\n",
    "    'Customer_ID': {'type': 'int', 'rule': lambda x: x > 0, 'null_allowed': False},\n",
    "    'Account_Balance': {'type': 'float', 'rule': lambda x: x >= 0, 'null_allowed': False},\n",
    "    'Transaction_Amount': {'type': 'float', 'rule': lambda x: x >= 0, 'null_allowed': False},\n",
    "    'Reported_Amount': {'type': 'float', 'rule': lambda x: x >= 0, 'null_allowed': False},\n",
    "    'Currency': {\n",
    "        'type': 'str',\n",
    "        'rule': lambda x: x in ['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY', 'INR', 'SGD'],  # Expanded list of ISO 4217 codes (source: https://www.iso.org/iso-4217-currency-codes.html)\n",
    "        'null_allowed': False\n",
    "    },\n",
    "    'Country': {'type': 'str', 'rule': lambda x: x in ['US', 'DE', 'UK', 'JP'], 'null_allowed': False},\n",
    "    'Transaction_Date': {\n",
    "        'type': 'datetime',\n",
    "        'rule': lambda x: pd.to_datetime(x) <= pd.Timestamp(datetime.datetime.now()) and (pd.Timestamp(datetime.datetime.now()) - pd.to_datetime(x)).days <= 365,\n",
    "        'null_allowed': False\n",
    "    },\n",
    "    'Risk_Score': {'type': 'float', 'rule': lambda x: 0 <= x <= 10 if pd.notnull(x) else True, 'null_allowed': True},\n",
    "    'Transaction_Amount vs Reported_Amount': {\n",
    "        'cross_field': lambda row: abs(row['Transaction_Amount'] - row['Reported_Amount']) <= 0.01 * row['Transaction_Amount'] if row.get('Cross_Currency', 'No') != 'No' else row['Transaction_Amount'] == row['Reported_Amount']\n",
    "    },\n",
    "    'Round_Number_Transaction': {'cross_field': lambda row: row['Transaction_Amount'] % 1000 != 0},\n",
    "    'High_Risk_Transaction': {'cross_field': lambda row: not (row['Transaction_Amount'] > 5000 and row['Country'] in ['DE'])}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a5b1c2-a740-4ff6-af54-bb7db6836304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset against profiling rules, checking data types, null values, and cross-field conditions\n",
    "def validate_dataset(df, rules):\n",
    "    validation_results = {}\n",
    "    for column, rule in rules.items():\n",
    "        if 'cross_field' in rule:\n",
    "            continue\n",
    "        if column not in df.columns:\n",
    "            validation_results[column] = f\"Column '{column}' not found in dataset.\"\n",
    "            continue\n",
    "        if rule['type'] == 'int' and not pd.api.types.is_integer_dtype(df[column]):\n",
    "            validation_results[column] = f\"Column '{column}' should be integer type.\"\n",
    "            continue\n",
    "        elif rule['type'] == 'float' and not pd.api.types.is_float_dtype(df[column]):\n",
    "            validation_results[column] = f\"Column '{column}' should be float type.\"\n",
    "            continue\n",
    "        elif rule['type'] == 'datetime':\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "        if not rule['null_allowed'] and df[column].isnull().any():\n",
    "            invalid_rows = df[df[column].isnull()].index.tolist()\n",
    "            validation_results[column] = f\"Column '{column}' contains null values at rows: {invalid_rows}\"\n",
    "            continue\n",
    "        if rule['null_allowed']:\n",
    "            invalid_rows = df[column][df[column].notnull() & ~df[column].apply(rule['rule'])].index.tolist()\n",
    "        else:\n",
    "            invalid_rows = df[column][~df[column].apply(rule['rule'])].index.tolist()\n",
    "        if invalid_rows:\n",
    "            validation_results[column] = f\"Column '{column}' has invalid values at rows: {invalid_rows}\"\n",
    "        else:\n",
    "            validation_results[column] = \"Valid\"\n",
    "    for column, rule in rules.items():\n",
    "        if 'cross_field' not in rule:\n",
    "            continue\n",
    "        invalid_rows = df[~df.apply(rule['cross_field'], axis=1)].index.tolist()\n",
    "        if invalid_rows:\n",
    "            validation_results[column] = f\"Cross-field validation failed at rows: {invalid_rows}\"\n",
    "        else:\n",
    "            validation_results[column] = \"Valid\"\n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd6ed51-2f2b-4a4a-96e8-381891a50c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies in the dataset using Isolation Forest, focusing on numerical features\n",
    "def detect_anomalies(df):\n",
    "    features = ['Account_Balance', 'Transaction_Amount', 'Reported_Amount']\n",
    "    X = df[features].fillna(0)  # Handle NaN values to prevent errors\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Adjusted contamination\n",
    "    df['Anomaly'] = iso_forest.fit_predict(X)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e96ffe8-ba91-41bc-bc40-432fff65301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust risk scores based on validation failures and anomalies, with configurable weights\n",
    "def adjust_risk_score(row, validation_results):\n",
    "    risk_adjustment = 0\n",
    "    for column, result in validation_results.items():\n",
    "        if 'rows' in str(result):\n",
    "            try:\n",
    "                invalid_rows = [int(r) for r in str(result).split('[')[1].split(']')[0].split(',') if r.strip().isdigit()]\n",
    "                if row.name in invalid_rows:\n",
    "                    risk_adjustment += 2  # Weight for validation failure\n",
    "            except (IndexError, ValueError):\n",
    "                pass\n",
    "    if row['Anomaly'] == -1:\n",
    "        risk_adjustment += 3  # Weight for anomaly\n",
    "    base_score = row['Risk_Score'] if pd.notnull(row['Risk_Score']) else 2.0  # Default to 2.0 if NaN\n",
    "    return base_score + risk_adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ece86d0-8cad-4587-8c07-fd07c51271aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset from CSV file for validation and analysis\n",
    "df = pd.read_csv('sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41079eb3-aeb9-4698-99ab-54dbbdc49f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset to ensure correct data types and formats for validation\n",
    "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], errors='coerce')\n",
    "df['Country'] = df['Country'].str.upper()\n",
    "df['Currency'] = df['Currency'].str.upper()\n",
    "# Convert numerical columns to float to fix type issues\n",
    "df['Account_Balance'] = pd.to_numeric(df['Account_Balance'], errors='coerce').astype(float)\n",
    "df['Transaction_Amount'] = pd.to_numeric(df['Transaction_Amount'], errors='coerce').astype(float)\n",
    "df['Reported_Amount'] = pd.to_numeric(df['Reported_Amount'], errors='coerce').astype(float)\n",
    "# Handle null values for required fields\n",
    "df['Currency'] = df['Currency'].fillna('USD')  # Default to USD\n",
    "df['Country'] = df['Country'].fillna('Unknown')  # Default to Unknown\n",
    "df['Transaction_Date'] = df['Transaction_Date'].fillna(pd.Timestamp('2025-01-01'))  # Default date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652a8c79-f694-4175-b62f-1478fe118176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'Customer_ID': 'Valid', 'Account_Balance': 'Valid', 'Transaction_Amount': 'Valid', 'Reported_Amount': 'Valid', 'Currency': 'Valid', 'Country': \"Column 'Country' has invalid values at rows: [2]\", 'Transaction_Date': 'Valid', 'Risk_Score': 'Valid', 'Transaction_Amount vs Reported_Amount': 'Cross-field validation failed at rows: [3]', 'Round_Number_Transaction': 'Cross-field validation failed at rows: [3]', 'High_Risk_Transaction': 'Valid'}\n"
     ]
    }
   ],
   "source": [
    "# Run validation on the dataset using the defined profiling rules and display results\n",
    "validation_results = validate_dataset(df, default_profiling_rules)\n",
    "print(\"Validation Results:\", validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ecae14-9d30-441e-839a-58e97daf0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Detection Results:\n",
      "    Customer_ID  Anomaly\n",
      "0         1001        1\n",
      "1         1002        1\n",
      "2         1003        1\n",
      "3         1004       -1\n"
     ]
    }
   ],
   "source": [
    "# Detect anomalies in the dataset and display the results\n",
    "df = detect_anomalies(df)\n",
    "print(\"Anomaly Detection Results:\\n\", df[['Customer_ID', 'Anomaly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da961364-0c49-4393-ba0b-0cc039d453e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Risk Scores:\n",
      "    Customer_ID  Risk_Score  Adaptive_Risk_Score  Anomaly\n",
      "0         1001         3.0                  3.0        1\n",
      "1         1002         2.0                  2.0        1\n",
      "2         1003         NaN                  4.0        1\n",
      "3         1004         5.0                 12.0       -1\n"
     ]
    }
   ],
   "source": [
    "# Apply adaptive risk scoring to the dataset and display the results\n",
    "df['Adaptive_Risk_Score'] = df.apply(lambda row: adjust_risk_score(row, validation_results), axis=1)\n",
    "print(\"Adaptive Risk Scores:\\n\", df[['Customer_ID', 'Risk_Score', 'Adaptive_Risk_Score', 'Anomaly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47311d30-3099-4677-bbb7-34f240f6b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remediation Suggestions:\n",
      "    Customer_ID                            Remediation_Suggestions\n",
      "0         1001                                                 []\n",
      "1         1002                                                 []\n",
      "2         1003  [Correct Country to a valid jurisdiction (e.g....\n",
      "3         1004  [Adjust Reported_Amount to match Transaction_A...\n"
     ]
    }
   ],
   "source": [
    "# Generate remediation suggestions for flagged transactions with detailed explanations\n",
    "def suggest_remediation(row, validation_results):\n",
    "    suggestions = []\n",
    "    for column, result in validation_results.items():\n",
    "        if 'rows' in str(result):\n",
    "            try:\n",
    "                invalid_rows = [int(r) for r in str(result).split('[')[1].split(']')[0].split(',') if r.strip().isdigit()]\n",
    "                if row.name in invalid_rows:\n",
    "                    if column == 'Transaction_Amount vs Reported_Amount':\n",
    "                        suggestions.append(f\"Adjust Reported_Amount to match Transaction_Amount within 1% deviation if cross-currency (Current: Transaction_Amount={row['Transaction_Amount']}, Reported_Amount={row['Reported_Amount']}).\")\n",
    "                    elif column == 'Currency':\n",
    "                        suggestions.append(\"Correct Currency to a valid ISO 4217 code (e.g., USD, EUR, GBP, JPY).\")\n",
    "                    elif column == 'Country':\n",
    "                        suggestions.append(\"Correct Country to a valid jurisdiction (e.g., US, DE, UK, JP).\")\n",
    "                    elif column == 'Transaction_Date':\n",
    "                        suggestions.append(\"Review Transaction_Date; ensure it’s not future-dated or over 365 days old.\")\n",
    "                    elif column == 'Round_Number_Transaction':\n",
    "                        suggestions.append(f\"Review transaction for potential money laundering risk due to round number (Transaction_Amount={row['Transaction_Amount']}).\")\n",
    "            except (IndexError, ValueError):\n",
    "                pass\n",
    "    if row['Anomaly'] == -1:\n",
    "        suggestions.append(\"Review transaction for potential anomalies in Account_Balance, Transaction_Amount, or Reported_Amount.\")\n",
    "    return suggestions\n",
    "\n",
    "df['Remediation_Suggestions'] = df.apply(lambda row: suggest_remediation(row, validation_results), axis=1)\n",
    "print(\"Remediation Suggestions:\\n\", df[['Customer_ID', 'Remediation_Suggestions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4588e-b8d0-4db5-ac4d-9faeb094e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa2a595563544078a07f4308a423f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07968b35ee1b406980a734b461fc9452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:  99%|#########9| 9.76G/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed19362c9418462299215fa9ebb67579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:  21%|##1       | 2.08G/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fce3ee48224e18b29c8c26ad6e416b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:  77%|#######6  | 7.57G/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use an LLM to generate profiling rules from regulatory instructions, with a fallback manual implementation\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load StarCoder with 8-bit quantization for CPU usage\n",
    "checkpoint = \"bigcode/starcoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Load model without 8-bit quantization for CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map={\"\": \"cpu\"},  # Force CPU usage\n",
    "    torch_dtype=torch.float32  # Use float32 since FP16 may not work on CPU\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True,\n",
    "    device=-1  # -1 for CPU (since your device is set to CPU)\n",
    ")\n",
    "\n",
    "instructions = \"\"\"\n",
    "Transaction_Amount should always match Reported_Amount, except when the transaction involves cross-currency conversions, in which case a permissible deviation of up to 1% is allowed.\n",
    "Currency should be a valid ISO 4217 currency code.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Convert these regulatory instructions into Python validation functions:\\n\\n{instructions}\\n\\nExample:\\ndef validate_amounts(row):\\n    if row['Cross_Currency'] == 'No':\\n        return row['Transaction_Amount'] == row['Reported_Amount']\\n    else:\\n        return abs(row['Transaction_Amount'] - row['Reported_Amount']) <= 0.01 * row['Transaction_Amount']\\n\\nProvide similar Python functions for the given instructions.\"\n",
    "\n",
    "try:\n",
    "    # Use max_new_tokens to specify the number of new tokens to generate, excluding the input prompt\n",
    "    generated_rules = generator(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "    print(\"Generated Rules:\\n\", generated_rules)\n",
    "    # Create the artifacts directory if it doesn't exist\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "    # Save the generated rules to a file with a timestamp for documentation\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    rules_filename = f'artifacts/generated_rules_{timestamp}.txt'\n",
    "    try:\n",
    "        with open(rules_filename, 'w') as f:\n",
    "            f.write(f\"# Generated Rules\\n# Timestamp: {timestamp}\\n# Prompt:\\n{prompt}\\n\\n\")\n",
    "            f.write(generated_rules)\n",
    "        print(f\"Generated rules saved to '{rules_filename}' for documentation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save generated rules to '{rules_filename}': {e}\")\n",
    "    # Check if the generated output contains a complete Python function definition\n",
    "    if \"def validate_\" in generated_rules and \"return\" in generated_rules and generated_rules.count('def') == generated_rules.count('return'):\n",
    "        print(\"Generated rules appear to contain complete Python functions. You can parse and use them.\")\n",
    "        summary_status = \"Usable\"\n",
    "        summary_filenames = f\"Generated: {rules_filename}\"\n",
    "    else:\n",
    "        print(\"Generated rules are not usable Python functions (incomplete or incorrect syntax). Using manual implementation instead.\")\n",
    "        manual_rules = \"\"\"\n",
    "def validate_amounts(row):\n",
    "    if row.get('Cross_Currency', 'No') == 'No':\n",
    "        return row['Transaction_Amount'] == row['Reported_Amount']\n",
    "    return abs(row['Transaction_Amount'] - row['Reported_Amount']) <= 0.01 * row['Transaction_Amount']\n",
    "\n",
    "def validate_currency(row):\n",
    "    iso_4217_codes = {'USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY', 'INR', 'SGD'}  # Expanded list\n",
    "    return row['Currency'] in iso_4217_codes\n",
    "        \"\"\"\n",
    "        print(\"Manual Rules (integrated into Cell 2):\\n\", manual_rules)\n",
    "        # Save the manual rules to a file for documentation\n",
    "        manual_rules_filename = f'artifacts/manual_rules_{timestamp}.txt'\n",
    "        try:\n",
    "            with open(manual_rules_filename, 'w') as f:\n",
    "                f.write(f\"# Manual Rules (Fallback)\\n# Timestamp: {timestamp}\\n# Used because generated rules were unusable\\n\\n\")\n",
    "                f.write(manual_rules)\n",
    "            print(f\"Manual rules saved to '{manual_rules_filename}' for documentation.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save manual rules to '{manual_rules_filename}': {e}\")\n",
    "        summary_status = \"Unusable (used manual rules)\"\n",
    "        summary_filenames = f\"Generated: {rules_filename}, Manual: {manual_rules_filename}\"\n",
    "except Exception as e:\n",
    "    print(f\"LLM generation failed: {e}\")\n",
    "    # Fallback: Manually implement the rules if LLM fails\n",
    "    print(\"Using manually implemented rules as fallback (integrated into Cell 2):\")\n",
    "    manual_rules = \"\"\"\n",
    "def validate_amounts(row):\n",
    "    if row.get('Cross_Currency', 'No') == 'No':\n",
    "        return row['Transaction_Amount'] == row['Reported_Amount']\n",
    "    return abs(row['Transaction_Amount'] - row['Reported_Amount']) <= 0.01 * row['Transaction_Amount']\n",
    "\n",
    "def validate_currency(row):\n",
    "    iso_4217_codes = {'USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY', 'INR', 'SGD'}  # Expanded list\n",
    "    return row['Currency'] in iso_4217_codes\n",
    "    \"\"\"\n",
    "    print(manual_rules)\n",
    "    # Save the manual rules to a file for documentation\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    manual_rules_filename = f'artifacts/manual_rules_{timestamp}.txt'\n",
    "    try:\n",
    "        with open(manual_rules_filename, 'w') as f:\n",
    "            f.write(f\"# Manual Rules (Fallback)\\n# Timestamp: {timestamp}\\n# Used because LLM generation failed\\n\\n\")\n",
    "            f.write(manual_rules)\n",
    "        print(f\"Manual rules saved to '{manual_rules_filename}' for documentation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save manual rules to '{manual_rules_filename}': {e}\")\n",
    "    summary_status = \"LLM Failed (used manual rules)\"\n",
    "    summary_filenames = f\"Manual: {manual_rules_filename}\"\n",
    "\n",
    "# Append a summary of this run to a log file\n",
    "summary_log_filename = 'artifacts/summary_log.txt'\n",
    "try:\n",
    "    with open(summary_log_filename, 'a') as f:\n",
    "        f.write(f\"Run at {timestamp}: Status: {summary_status}, Files: {summary_filenames}\\n\")\n",
    "    print(f\"Summary logged to '{summary_log_filename}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log summary to '{summary_log_filename}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
